\pagenumbering{arabic}
\section{绪论}
\subsection{研究背景}
21世纪是一个信息化时代，根据维基百科的定义，信息化时代是每个人都有能力去自由的传播信息，并且能够及时的获取信息的时代。信息化时代是以计算机技术及网络技术为核心，让信息的传播建立在网络及电子设备上。在信息化时代，为了满足时代的要求，获取大量且具有时效性的”信息“则变得非常重要。

随着互联网的发展，人们正处于一个信息爆炸的时代。相比于过去的信息匮乏，面对现阶段海量的信息数据，人们淹没在数据中难以快速的制定合适的决策。

面对上述的问题，人们开始思考如何从海量数据集中获取用户的信息和知识，然而面对高维、复杂、异构的海量数据，提取有用的潜在信息成为一大难题。面对这个难题，数据挖掘技术应运而生。

众所周知，对信息的筛选和过滤是衡量一个系统好坏的重要指标。一个具有良好用户体验的系统，会将海量信息进行筛选、过滤，将用户最关注最感兴趣的信息展现在用户面前。这大大增加了系统工作的效率，也节省了用户筛选信息的时间。

其中，使用数据处理工具进行数据处理其在用户对信息选择以及依据用户来刷选有用信息方面有很大的使用几率。降低过量数据给人们选择带来的困扰，减少用户选择成本，提高有用信息数据的获取几率。比如说在电子商务方面，如何让消费者在过多商品信息中选择出自己想要的商品以及自己潜在需求的商品，数据处理就能很好的解决这个问题。

\subsection{研究需求与意义}
搜索引擎的出现在一定程度上解决了信息筛选问题，能有效地实现从海量数据及中获取有用信息和知识，但还远远不够。搜索引擎需要用户主动提供关键词来对海量信息进行筛选。当用户无法准确描述自己的需求时，搜索引擎的筛选效果将大打折扣，而用户将自己的需求和意图转化成关键词的过程本身就是一个并不轻松的过程。

在这个时代，无论是信息消费者还是信息生产者都遇到了很大的挑战：对于信息消费者，从大量信息中找到自己感兴趣的信息是一件非常困难的事情；对于信息生产者，让自己生产的信息脱颖而出，受到广大用户的关注，也是一件非常困难的事情。推荐系统就是解决这一矛盾的重要工具。推荐系统的任务就是联系用户和信息，一方面帮助用户发现对自己有价值的信息，另一方面让信息能够展现在对它感兴趣的用户面前，从而实现信息消费者和信息生产者的双赢。

\subsection{常用数据挖掘软件简介}
商用数据挖掘软件比较著名的有SPSS Clementine、IBM  Intelligent Miner、SAS Enterprise Miner等等，他们都能提供常规的挖掘过程和挖掘模式。Excel、Matlab等提供了数据挖掘模块。开源数据挖掘工具主要有WEKA、RapidMiner、ARMiner和AlphaMiner等。

本文中采用RapidMiner软件对数据进行处理，接下来，我们主要介绍一下RapidMiner这款软件。

RapidMiner也称为YALE（Yet Another Learning Environment，https://rapidminer.com），提供图形化界面，采用类似Windows资源管理器中的树状结构来组织分析组件，树上每个节点表示不同的运算符（operator）。YALE中提供了大量的运算符，包括数据处理、变换、探索、建模、评估等各个环节。YALE是用Java开发的，基于 Weka 来构建，可以调用 Weka 中的各种分析组件。RapidMiner 有拓展的套件 Rhadoop，可以和Hadoop集成起来，在Hadoop集群上运行任务。

RapidMiner Studio 结合技术性和适用性，为最新的及已建立的人性化数据挖掘技术提供服务。通过推拽算子，设置参数及组合算子，在RapidMiner Studio中定义分析流程。 流程能从大量的随机的可嵌套的算子中产生，最终表示为所谓的流程图（流程设计）。流程结构由内部的XML来描述，通过图形用户界面来开发。在后台，RapidMiner Studio 不断地检查当前流程开发状态，确保语法一致，并在问题出现时，能自动推荐解决方案。以上功能是通过所谓的元数据转换实现的，即在流程设计阶段转换基础元数据，预知流程开发结果，并在出现不合适的算子组合时确定解决方案（快速修复）。此外，RapidMiner Studio也能定义断点，因此能检查几乎所有的中间结果。成功组合的算子会被合并到构建模块中，因此在后期流程中它们还能被再次使用。
通过YALE，我们可以检测数据是否存在错误以及在建模过程中出现的问题，及时找出问题并加以解决。


\subsection{研究现状及其前景}
\subsubsection{海量数据处理的主要方法的研究现状}

主流的海量数据处理技术基本上可以归结为两步，即：数据质量分析和数据的特征分析方法。数据质量分析是数据挖掘中数据准备过程的重要一环，是数据预处理的前提，也是数据挖掘分析结论有效性和准确性的基础，没有可信的数据，数据挖掘构建的模型将是空中楼阁。数据质量分析的主要任务是检查原始数据中是否存在脏数据，脏数据一般是指不符合要求，以及不能直接进行相应分析的数据。对数据进行质量分析以后，接下来可通过绘制图表、计算某些特征量等手段进行数据的特征分析。

%\textbf{因为你这里用了enumerate 所以 enumerate 中 的item没有缩进，大段文字不需要这样处理}

%\begin{enumerate}[label = (\arabic*), itemindent = 1.5\parindent]
(1)分布分析 

 分布分析能揭示数据的分布特征和分布类型。对于定量数据，欲了解其分布形式是对称的还是非对称的、发现某些特大或特小的可疑值，可做出频率分布表、绘制频率分布直方图、绘制茎叶图进行直观地分析；对于定性分类数据，可用饼图和条形图直观地显示分布情况。

(2)对比分析

对比分析是指把两个相互联系的指标进行比较，从数量上展示和说明研究对象规模的大小，水平的高低，速度的快慢，以及各种关系是否协调。特别适用于指标间的横纵向比较、时间序列的比较分析。在对比分析中，选择合适的对比标准是十分关键的步骤，选择得合适，才能做出客观的评价，选择不合适，评价可能得出错误的结论。

(3)统计量分析 

 用统计指标对定量数据进行统计描述，常从集中趋势和离中趋势两个方面进行分析。

 平均水平的指标是对个体集中趋势的度量，使用最广泛的是均值和中位数；反映变异程度的指标则是对个体离开平均水平的度量，使用较广泛的是标准差（方差）、四分位间距。

(4)周期性分析 

 周期性分析是探索某个变量是否随着时间变化而呈现出某种周期变化趋势。时间尺度相对较长的周期性趋势有年度周期性趋势、季节性周期趋势，相对较短的有月度周期性趋势、周度周期性趋势，甚至更短的天、小时周期性趋势。

(5)贡献度分析 

 贡献度分析又称帕累托分析，它的原理是帕累托法则又称20/80定律。同样的投入放在不同的地方会产生不同的效益。比如对一个公司来讲，80\%的利润常常来自于20\%最畅销的产品，而其他80\%的产品只产生了20\%的利润。

(6)相关性分析 

 分析连续变量之间线性相关程度的强弱，并用适当的统计指标表示出来的过程称为相关分析。
%\end{enumerate}
\subsubsection{大数据的未来发展}

而大数据未来的发展趋势则从以下几个方面进行：



(1)开放源代码

大数据获得动力，关键在于开放源代码，帮助分解和分析数据。Hadoop 和NoSQL 数据库便是其中的赢家，他们让其他技术商望而却步、处境很被动。毕竟，我们需要清楚怎样创建一个平台，既能解开所有的数据，克服数据相互独立的障碍，又能将数据重新上锁。

(2)市场细分

当今，许多通用的大数据分析平台已投入市场，人们同时期望更多平台的出现，可以运用在特殊领域，如药物创新、客户关系管理、应用性能的监控和使用。若市场逐步成熟，在通用分析平台之上，开发特定的垂直应用将会实现。但现在的技术有限，除非考虑利用潜在的数据库技术作为通用平台 ( 如Hadoop、NoSQL)。人们期望更多特定的垂直应用出现，把目标定为特定领域的数据分析，这些特定领域包括航运业、销售业、网上购物、社交媒体用户的情绪分析等。同时，其他公司正在研发小规模分析引擎的软件套件。比如，社交媒体管理工具，这些工具以数据分析做为基础。

(3)预测分析

建模、机器学习、统计分析和大数据经常被联系起来，用以预测即将发生的事情和行为。有些事情是很容易被预测的，比如坏天气可以影响选民的投票率，但是有些却很难被准确预测。例如，中间选民改变投票决定的决定性因素。但是，当数据累加时，我们基本上有能力可以大规模尝试一个连续的基础。网上零售商重新设计购物车，来探索何种设计方式能使销售利润最大化。根据病人的饮食、家族史和每天的运动量，医生有能力预测未来疾病的风险。当然，在人类历史的开端，我们就已经有各种预测。但是，在过去，许多预测都是基于直觉，没有依靠完整的数据集，或者单单靠的是常识。当然，即便有大量数据支撑你的预测，也不表明那些预测都是准确的。2007 年和2008 年，许多对冲基金经理和华尔街买卖商分析市场数据，认为房地产泡沫将不会破灭。根据历史的数据，可以预测出房地产泡沫即将破裂，但是许多分析家坚持原有的观点。另一方面，预测分析在许多领域流行起来，例如欺诈发现( 比如在外省使用信用卡时会接到的诈骗电话)，保险公司和顾客维系的风险管理。




